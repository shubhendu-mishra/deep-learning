{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Networks **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How computers see images? In Arrays, IN RGB or Grayscale values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization:** We scale the data between 0 and 1\n",
    "**Flattening**: 4x4 mattrix is flattened with each row next to each other\n",
    "\n",
    "Imagine the matrix as follows:\n",
    "\n",
    "        1 a 2 b\n",
    "        3 c 4 d\n",
    "        5 e 6 f\n",
    "        \n",
    "After flattening it will become:\n",
    "\n",
    "        1 a 2 b 3 c 4 d 5 e 6 f\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White Pixel - 255, Black Pixel - 0 , Normalized between 0 to 1- as these networks have to see how important the pixel is\n",
    "\n",
    "If image  is 28x28 - The flattened image vector will have 784 entries in a single row. So the input to a Multilayer Perceptron will be the vector with 784 entries. \n",
    "\n",
    "The output will have classes suppose 10. Class scores will be given. Class scores mean how certain the network is, the more the score, the sure the network is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be any number of hidden layers. Very high can lead to overfitting.\n",
    "Loss function calculates difference between true class and predicted class.\n",
    "Backpropagation checks how bad the weights are in making this prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Function** is applied to convert scores intro probabilites.\n",
    "**Cross Entropy Loss** Categorical Class outputs so - Negative log is taken of the probability. \n",
    "Cross Entropy Loss is inversely propotional to probability, so \n",
    "\n",
    "                                C E Loss is Lower when true label and prediction agree\n",
    "                                C E Loss is Higher when true label and prediction disagree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer** tries to minimize the loss ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Size** is the number of images the network is taking at a time and trying to learn from, **Number of workers** is the simultaneous threads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloaders** help iterate the data one batch at a time,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, we have to define and name any layers in our network in __init__ function,then we have to define **forward** function and forward pass\n",
    "We apply (Rectified Linear Unit) relu to outputs of hidden layers so that the outputs are positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our model and loading our data, the next thing is Loss and Optimization function.\n",
    "**Loss function** recommended for classification is Cross Entropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epochs** means how many times the model is seeing the whole training dataset, 1 time means the model is seeing all images only once. **Recommended** to have epochs between 20-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch loader** comes within the Epoch\n",
    "The optimizer step is responsible for updating the weight values in our network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Cross Entropy Loss Function** actually involves two steps:\n",
    "\n",
    "1. It first applies a softmax function to any output is sees\n",
    "2. Then applies NLLLoss; negative log likelihood loss\n",
    "\n",
    "Then it returns the average loss over a batch of data. \n",
    "\n",
    "Since it applies a softmax function, we do not have to specify that in the forward function of our model definition, but we could do this another way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm trying **MLP(mnist_mlp_exercise.ipynb)**,\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
